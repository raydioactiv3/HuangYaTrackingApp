{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raydioactiv3/HuangYaTrackingApp/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Fine-tuning Florence-2 on Object Detection Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/florence-2/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)\n",
        "\n",
        "Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. The model demonstrates strong zero-shot and fine-tuning capabilities across tasks such as captioning, object detection, grounding, and segmentation.\n",
        "\n",
        "![Florence-2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-1.png)\n",
        "\n",
        "*Figure 1. Illustration showing the level of spatial hierarchy and semantic granularity expressed by each task. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n",
        "The model takes images and task prompts as input, generating the desired results in text format. It uses a DaViT vision encoder to convert images into visual token embeddings. These are then concatenated with BERT-generated text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response.\n",
        "\n",
        "![Florence-2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-2.png)\n",
        "\n",
        "*Figure 2. Overview of Florence-2 architecture. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n"
      ],
      "metadata": {
        "id": "KVmYbqqUSlCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "z16cfHRE8yi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ],
      "metadata": {
        "id": "mqd30Ndg9dbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `L4 GPU`, and then click `Save`."
      ],
      "metadata": {
        "id": "n32nrwCeAEYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMmBuhiiC2mX",
        "outputId": "68452ad9-89d8-4188-e560-028754e0affc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 29 03:47:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   36C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download example data\n",
        "\n",
        "**NOTE:** Feel free to replace our example image with your own photo."
      ],
      "metadata": {
        "id": "dOshHQM3Ebq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ZhBCTPEnEO",
        "outputId": "540faae4-59b6-43d1-cf5e-878e1b90e539"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108K\n",
            "-rw-r--r-- 1 root root 104K Jun  2  2023 dog.jpeg\n",
            "drwxr-xr-x 1 root root 4.0K May 23 13:39 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLE_IMAGE_PATH = \"dog.jpeg\""
      ],
      "metadata": {
        "id": "PbglpBOOFCHm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and configure the model\n",
        "\n",
        " Let's download the model checkpoint and configure it so that you can fine-tune it later on."
      ],
      "metadata": {
        "id": "GM4QlaUfCFsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers flash_attn timm einops peft\n",
        "!pip install -q roboflow git+https://github.com/roboflow/supervision.git"
      ],
      "metadata": {
        "id": "Y6b1dvjgYXOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b351ccd-b50c-4152-bf58-1afa0fac3551"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m179.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for supervision (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import html\n",
        "import base64\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from google.colab import userdata\n",
        "from IPython.core.display import display, HTML\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.AdamW\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    get_scheduler\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any, Tuple, Generator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "from roboflow import Roboflow"
      ],
      "metadata": {
        "id": "HMd6tb4sSh9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "e6065e54-4f18-4572-c3fd-51b8aef2f4f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fb04d3c7db30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor` classes from the transformers library. Note that you need to pass `trust_remote_code` as `True` since this model is not a standard transformers model."
      ],
      "metadata": {
        "id": "flp13B-8Myjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
        "REVISION = 'refs/pr/6'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION).to(DEVICE)\n",
        "processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION)"
      ],
      "metadata": {
        "id": "zqDWEWDcaSxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference with pre-trained Florence-2 model"
      ],
      "metadata": {
        "id": "rf1GlvvQFec-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example object detection inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "ReAKWNxAFmv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example image captioning inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<DETAILED_CAPTION>\"\n",
        "text = \"<DETAILED_CAPTION>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "response"
      ],
      "metadata": {
        "id": "eSLSErXeI84L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example caption to phrase grounding inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
        "text = \"<CAPTION_TO_PHRASE_GROUNDING> In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "PrVSTmFPJjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Florence-2 on custom dataset"
      ],
      "metadata": {
        "id": "eQetrQM7Jziy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset from Roboflow Universe"
      ],
      "metadata": {
        "id": "Sw7D6ZYzAs9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "\n",
        "project = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"florence2-od\")"
      ],
      "metadata": {
        "id": "K1IlyjYmBCxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 {dataset.location}/train/annotations.jsonl"
      ],
      "metadata": {
        "id": "iiLclUnKTrLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define `DetectionsDataset` class\n",
        "\n",
        "class JSONLDataset:\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.jsonl_file_path = jsonl_file_path\n",
        "        self.image_directory_path = image_directory_path\n",
        "        self.entries = self._load_entries()\n",
        "\n",
        "    def _load_entries(self) -> List[Dict[str, Any]]:\n",
        "        entries = []\n",
        "        with open(self.jsonl_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data = json.loads(line)\n",
        "                entries.append(data)\n",
        "        return entries\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:\n",
        "        if idx < 0 or idx >= len(self.entries):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
        "        try:\n",
        "            image = Image.open(image_path)\n",
        "            return (image, entry)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n",
        "\n",
        "\n",
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, data = self.dataset[idx]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        return prefix, suffix, image"
      ],
      "metadata": {
        "id": "dExvJNFkxymc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    return inputs, answers\n",
        "\n",
        "train_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/train/\"\n",
        ")\n",
        "val_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/valid/\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "ilMb0ivGdt9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup LoRA Florence-2 model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    inference_mode=False,\n",
        "    use_rslora=True,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    revision=REVISION\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "FmPJOXCzB-29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1V9BcVQMycgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run inference with pre-trained Florence-2 model on validation dataset\n",
        "\n",
        "def render_inline(image: Image.Image, resize=(128, 128)):\n",
        "    \"\"\"Convert image into inline html.\"\"\"\n",
        "    image.resize(resize)\n",
        "    with io.BytesIO() as buffer:\n",
        "        image.save(buffer, format='jpeg')\n",
        "        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
        "        return f\"data:image/jpeg;base64,{image_b64}\"\n",
        "\n",
        "\n",
        "def render_example(image: Image.Image, response):\n",
        "    try:\n",
        "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n",
        "        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n",
        "    except:\n",
        "        print('failed to redner model response')\n",
        "    return f\"\"\"\n",
        "<div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" />\n",
        "    <p style=\"width:512px; margin:10px; font-size:small;\">{html.escape(json.dumps(response))}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def render_inference_results(model, dataset: DetectionDataset, count: int):\n",
        "    html_out = \"\"\n",
        "    count = min(count, len(dataset))\n",
        "    for i in range(count):\n",
        "        image, data = dataset.dataset[i]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3\n",
        "        )\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "        answer = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "        html_out += render_example(image, answer)\n",
        "\n",
        "    display(HTML(html_out))\n",
        "\n",
        "render_inference_results(peft_model, val_dataset, 4)"
      ],
      "metadata": {
        "id": "i9LEEXRwN9cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Florence-2 on custom object detection dataset"
      ],
      "metadata": {
        "id": "RH9JTq_RytE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define train loop\n",
        "\n",
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(\n",
        "                text=answers,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                return_token_type_ids=False\n",
        "            ).input_ids.to(DEVICE)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(\n",
        "                    text=answers,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    return_token_type_ids=False\n",
        "                ).input_ids.to(DEVICE)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "            render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "bC06Mc7jOdpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run train loop\n",
        "\n",
        "%%time\n",
        "\n",
        "EPOCHS = 10\n",
        "LR = 5e-6\n",
        "\n",
        "train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)"
      ],
      "metadata": {
        "id": "LZybGHd3fNJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuned model evaluation"
      ],
      "metadata": {
        "id": "MBHMu7WGWpeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check if the model can still detect objects outside of the custom dataset\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = peft_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ],
      "metadata": {
        "id": "xspOKfgRY2gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** It seems that the model can still detect classes that don't belong to our custom dataset."
      ],
      "metadata": {
        "id": "3Ygh9VyZaqnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Collect predictions\n",
        "\n",
        "PATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)<loc_\\d+>'\n",
        "\n",
        "def extract_classes(dataset: DetectionDataset):\n",
        "    class_set = set()\n",
        "    for i in range(len(dataset.dataset)):\n",
        "        image, data = dataset.dataset[i]\n",
        "        suffix = data[\"suffix\"]\n",
        "        classes = re.findall(PATTERN, suffix)\n",
        "        class_set.update(classes)\n",
        "    return sorted(class_set)\n",
        "\n",
        "CLASSES = extract_classes(train_dataset)\n",
        "\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(val_dataset.dataset)):\n",
        "    image, data = val_dataset.dataset[i]\n",
        "    prefix = data['prefix']\n",
        "    suffix = data['suffix']\n",
        "\n",
        "    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    prediction = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n",
        "    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n",
        "    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n",
        "    prediction.confidence = np.ones(len(prediction))\n",
        "\n",
        "    target = processor.post_process_generation(suffix, task='<OD>', image_size=image.size)\n",
        "    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n",
        "    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n",
        "\n",
        "    targets.append(target)\n",
        "    predictions.append(prediction)"
      ],
      "metadata": {
        "id": "8f1BYeQw3xhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate mAP\n",
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        ")\n",
        "\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ],
      "metadata": {
        "id": "88VnIZ_feHPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculate Confusion Matrix\n",
        "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "_ = confusion_matrix.plot()"
      ],
      "metadata": {
        "id": "85APzNRfe8xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save fine-tuned model on hard drive"
      ],
      "metadata": {
        "id": "8rR2naNXzEB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.save_pretrained(\"/content/florence2-lora\")\n",
        "processor.save_pretrained(\"/content/florence2-lora/\")\n",
        "!ls -la /content/florence2-lora/"
      ],
      "metadata": {
        "id": "Rdbmcv3TcIe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload model to Roboflow (optional)\n",
        "\n",
        "You can deploy your Florence-2 object detection model on your own hardware (i.e. a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\n",
        "\n",
        "To deploy your model, you will need a [free Roboflow account](https://app.roboflow.com).\n",
        "\n",
        "To get started, [create a new Project in Roboflow](https://docs.roboflow.com/datasets/create-a-project) if you don't already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\n",
        "\n",
        "You can read our full [Deploy Florence-2 with Roboflow](https://blog.roboflow.com/deploy-florence-2-with-roboflow/) guide for step-by-step instructions of these steps.\n",
        "\n",
        "Once you have trained your model A, you can upload it to Roboflow using the following code:"
      ],
      "metadata": {
        "id": "ZZ9q1Wa8FOg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"API_KEY\")\n",
        "project = rf.workspace(\"workspace-id\").project(\"project-id\")\n",
        "version = project.version(VERSION)\n",
        "\n",
        "version.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")"
      ],
      "metadata": {
        "id": "e1WLCEEBF2jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, replace:\n",
        "\n",
        "- API_KEY with your [Roboflow API key](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n",
        "- workspace-id and project-id with your [workspace and project IDs](https://docs.roboflow.com/api-reference/workspace-and-project-ids).\n",
        "- VERSION with your project version.\n",
        "\n",
        "If you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\n",
        "\n",
        "When you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\n",
        "\n",
        "Your model will be uploaded to Roboflow.\n",
        "\n",
        "## Deploy to your hardware\n",
        "\n",
        "Once your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\n",
        "\n",
        "Inference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\n",
        "\n",
        "For this guide, we will show how to deploy the model with the Python SDK.\n",
        "\n",
        "First, install inference:"
      ],
      "metadata": {
        "id": "CLSODWkmGANb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inference"
      ],
      "metadata": {
        "id": "jPDfMo8DGSsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, create a new Python file and add the following code:"
      ],
      "metadata": {
        "id": "7hShwaXwGXyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from inference import get_model\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "lora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n",
        "\n",
        "image = Image.open(\"containers.png\")\n",
        "response = lora_model.infer(image)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Qocv_VtQGVK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\n",
        "\n",
        "When you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
      ],
      "metadata": {
        "id": "o6knGuTaGZ-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations\n",
        "\n",
        "⭐️ If you enjoyed this notebook, [**star the Roboflow Notebooks repo**](https://https://github.com/roboflow/notebooks) (and [**supervision**](https://github.com/roboflow/supervision) while you're at it) and let us know what tutorials you'd like to see us do next. ⭐️"
      ],
      "metadata": {
        "id": "ag0XROk7fcd_"
      }
    }
  ]
}